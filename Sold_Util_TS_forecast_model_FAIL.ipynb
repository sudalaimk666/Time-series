{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53b3dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:10.149869Z",
     "start_time": "2022-04-25T19:08:08.465070Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "mpl.rcParams['figure.figsize'] = (10, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95532b",
   "metadata": {},
   "source": [
    "# Initialize Spark Session\n",
    "## we are telling like to run a spark session in a local mode and assign it to an object. when we run this in a cluster mode then we can distribute the thousands of products across hundreds of nodes for fast query execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1119df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:/spark/spark-3.0.3-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1322204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:20.906497Z",
     "start_time": "2022-04-25T19:08:10.153732Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2676\\1557518094.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167b430b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:20.937977Z",
     "start_time": "2022-04-25T19:08:20.908517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(882, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Utilized_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824d8683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:20.969891Z",
     "start_time": "2022-04-25T19:08:20.940966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>LOC</th>\n",
       "      <th>RING</th>\n",
       "      <th>Sold_bandwidth</th>\n",
       "      <th>Utilized_bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15-06-2021</td>\n",
       "      <td>chennai to kadappa</td>\n",
       "      <td>EIR</td>\n",
       "      <td>114.2</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-06-2021</td>\n",
       "      <td>chennai to chennai vsb</td>\n",
       "      <td>EIR</td>\n",
       "      <td>119.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-06-2021</td>\n",
       "      <td>chennai to hyderabad</td>\n",
       "      <td>EIR</td>\n",
       "      <td>106.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15-06-2021</td>\n",
       "      <td>hyderabad to kadappa 1</td>\n",
       "      <td>GCIR</td>\n",
       "      <td>116.8</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15-06-2021</td>\n",
       "      <td>kadappa to mumbai</td>\n",
       "      <td>GCIR</td>\n",
       "      <td>72.7</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                     LOC  RING  Sold_bandwidth  \\\n",
       "0  15-06-2021      chennai to kadappa   EIR           114.2   \n",
       "1  15-06-2021  chennai to chennai vsb   EIR           119.1   \n",
       "2  15-06-2021    chennai to hyderabad   EIR           106.9   \n",
       "3  15-06-2021  hyderabad to kadappa 1  GCIR           116.8   \n",
       "4  15-06-2021       kadappa to mumbai  GCIR            72.7   \n",
       "\n",
       "   Utilized_bandwidth  \n",
       "0                 9.7  \n",
       "1                 1.9  \n",
       "2                 0.4  \n",
       "3                 4.5  \n",
       "4                 5.4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a47a35",
   "metadata": {},
   "source": [
    "# Basic Preprocessing\n",
    "## The first is to convert the date column from string type to Date type. After that let us check that is there any missing values and how many unique values we have in each column? Basically when we are running a Multiple time series then we are going to treat each LOC as separate time series so we are going to build 21 different models over here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b560586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:20.999810Z",
     "start_time": "2022-04-25T19:08:20.972882Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa56d8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T19:08:21.030727Z",
     "start_time": "2022-04-25T19:08:21.002801Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " Index(['Date', 'LOC', 'RING', 'Sold_bandwidth', 'Utilized_bandwidth'], dtype='object')\n",
      "\n",
      "Missing values - \n",
      " Date                  False\n",
      "LOC                   False\n",
      "RING                  False\n",
      "Sold_bandwidth        False\n",
      "Utilized_bandwidth     True\n",
      "dtype: bool\n",
      "\n",
      "Unique values: \n",
      " Date                   42\n",
      "LOC                    21\n",
      "RING                    7\n",
      "Sold_bandwidth        238\n",
      "Utilized_bandwidth    150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Features:\\n\", df.columns)\n",
    "print(\"\\nMissing values - \\n\", df.isnull().any())\n",
    "print(\"\\nUnique values: \\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319608e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T16:18:03.586919Z",
     "start_time": "2022-04-26T16:18:02.437991Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecea1a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T13:56:44.066426Z",
     "start_time": "2022-04-26T13:56:44.039500Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['LOC', 'RING']] = df[['LOC', 'RING']].apply(LabelEncoder().fit_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f84de0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:08.489684Z",
     "start_time": "2022-04-25T08:28:08.460762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>LOC</th>\n",
       "      <th>RING</th>\n",
       "      <th>Sold_bandwidth</th>\n",
       "      <th>Utilized_bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>114.2</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>106.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>116.8</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>72.7</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  LOC  RING  Sold_bandwidth  Utilized_bandwidth\n",
       "0 2021-06-15    3     0           114.2                 9.7\n",
       "1 2021-06-15    1     0           119.1                 1.9\n",
       "2 2021-06-15    2     0           106.9                 0.4\n",
       "3 2021-06-15   10     1           116.8                 4.5\n",
       "4 2021-06-15   16     1            72.7                 5.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c98aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:08.504643Z",
     "start_time": "2022-04-25T08:28:08.493676Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Utilized_bandwidth'].fillna(df['Utilized_bandwidth'].mean(), inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "878da19f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:08.535562Z",
     "start_time": "2022-04-25T08:28:08.509632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                  0\n",
       "LOC                   0\n",
       "RING                  0\n",
       "Sold_bandwidth        0\n",
       "Utilized_bandwidth    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9f851",
   "metadata": {},
   "source": [
    "# Visualize the Time-series data\n",
    "## Now to visualize the time-series data of each location we are setting the date as an index and plot the graph for a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82d41bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:09.283952Z",
     "start_time": "2022-04-25T08:28:09.268993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1,  2, 10, 16, 13, 11, 17,  5,  7,  8,  9, 19,  4, 20, 15, 14,\n",
       "        0, 18, 12,  6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.LOC.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f1323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1ffe347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T10:51:35.848477Z",
     "start_time": "2022-04-24T10:51:29.902236Z"
    }
   },
   "source": [
    "for i in range(1,22):\n",
    "    item_df = df.set_index('Date')\n",
    "    item_df.query('LOC == \"{i}\" ')[['Sold_bandwidth', 'Utilized_bandwidth']].plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3d37e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:10.982298Z",
     "start_time": "2022-04-25T08:28:10.351857Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "item_df = df.set_index('Date')\n",
    "item_df.query('LOC == 1')[['Sold_bandwidth', 'Utilized_bandwidth']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9bde628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:13.863427Z",
     "start_time": "2022-04-25T08:28:13.841486Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                  0\n",
       "LOC                   0\n",
       "RING                  0\n",
       "Sold_bandwidth        0\n",
       "Utilized_bandwidth    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec944d",
   "metadata": {},
   "source": [
    "# Analyze the Dataset with Spark\n",
    "## Spark DataFrame\n",
    "\n",
    "## We have Pandas Dataframe and we have analyzed it. Now we will create a spark component so we will create a Spark DataFrame. you can also print schema of it to see the data type of columns. count function is used to count the rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f603b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:31:19.044088Z",
     "start_time": "2022-04-25T08:31:19.001203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- LOC: long (nullable = true)\n",
      " |-- RING: long (nullable = true)\n",
      " |-- Sold_bandwidth: double (nullable = true)\n",
      " |-- Utilized_bandwidth: double (nullable = true)\n",
      "\n",
      "+-------------------+---+----+--------------+------------------+\n",
      "|               Date|LOC|RING|Sold_bandwidth|Utilized_bandwidth|\n",
      "+-------------------+---+----+--------------+------------------+\n",
      "|2021-06-15 00:00:00|  3|   0|         114.2|               9.7|\n",
      "|2021-06-15 00:00:00|  1|   0|         119.1|               1.9|\n",
      "|2021-06-15 00:00:00|  2|   0|         106.9|               0.4|\n",
      "|2021-06-15 00:00:00| 10|   1|         116.8|               4.5|\n",
      "|2021-06-15 00:00:00| 16|   1|          72.7|               5.4|\n",
      "+-------------------+---+----+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df)\n",
    "sdf.printSchema() #data type of each col\n",
    "sdf.show(5) #It gives you head of pandas DataFrame\n",
    "sdf.count() #500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5219ac05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:28:15.818653Z",
     "start_time": "2022-04-25T08:28:15.772775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|LOC|count(LOC)|\n",
      "+---+----------+\n",
      "| 19|        42|\n",
      "|  0|        42|\n",
      "|  7|        42|\n",
      "|  6|        42|\n",
      "|  9|        42|\n",
      "| 17|        42|\n",
      "|  5|        42|\n",
      "|  1|        42|\n",
      "| 10|        42|\n",
      "|  3|        42|\n",
      "| 12|        42|\n",
      "|  8|        42|\n",
      "| 11|        42|\n",
      "|  2|        42|\n",
      "|  4|        42|\n",
      "| 13|        42|\n",
      "| 18|        42|\n",
      "| 14|        42|\n",
      "| 15|        42|\n",
      "| 20|        42|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(['LOC']).groupby('LOC').agg({'LOC': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18d2f4",
   "metadata": {},
   "source": [
    "### Now we will create a Temporary view to run the SQL queries on the dataframe. After this, we run a SQL query to find the count of Location and print it according to Locatrion. It will display the same table as shown in the above figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95814214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:13.448814Z",
     "start_time": "2022-04-24T11:36:09.959334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|LOC|count(1)|\n",
      "+---+--------+\n",
      "|  0|      42|\n",
      "|  1|      42|\n",
      "|  2|      42|\n",
      "|  3|      42|\n",
      "|  4|      42|\n",
      "|  5|      42|\n",
      "|  6|      42|\n",
      "|  7|      42|\n",
      "|  8|      42|\n",
      "|  9|      42|\n",
      "| 10|      42|\n",
      "| 11|      42|\n",
      "| 12|      42|\n",
      "| 13|      42|\n",
      "| 14|      42|\n",
      "| 15|      42|\n",
      "| 16|      42|\n",
      "| 17|      42|\n",
      "| 18|      42|\n",
      "| 19|      42|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView(\"Sold_bandwidth\")\n",
    "spark.sql(\"select LOC, count(*) from Sold_bandwidth group by LOC order by LOC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5263af",
   "metadata": {},
   "source": [
    "### Spark is a distributed framework so what happens is multiple executors are running to read a chunk of data. we need to manually tell that our chunk of data is LOC so that all the data related to location are in one partition and when a model is built it uses all the LOCs together. If we dont do this then different LOCs will be in different partitions. So we have to do it manually so we are running a below SQL statement in which we take a sum of Sold_bandwidth(It will not do anything because we have data at the week level). The prophet expects the date as ds and the target column as Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a414329b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:16.937808Z",
     "start_time": "2022-04-24T11:36:13.451807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+\n",
      "|LOC|                 ds|   y|\n",
      "+---+-------------------+----+\n",
      "|  0|2021-06-15 00:00:00|3.02|\n",
      "|  0|2021-06-22 00:00:00| 2.1|\n",
      "|  0|2021-06-29 00:00:00| 3.1|\n",
      "|  0|2021-07-06 00:00:00|3.02|\n",
      "|  0|2021-07-13 00:00:00|3.02|\n",
      "|  0|2021-07-20 00:00:00| 2.1|\n",
      "|  0|2021-07-27 00:00:00| 2.5|\n",
      "|  0|2021-08-03 00:00:00| 2.1|\n",
      "|  0|2021-08-10 00:00:00| 2.1|\n",
      "|  0|2021-08-17 00:00:00| 2.1|\n",
      "|  0|2021-08-24 00:00:00| 2.1|\n",
      "|  0|2021-08-31 00:00:00| 2.1|\n",
      "|  0|2021-09-07 00:00:00| 2.1|\n",
      "|  0|2021-09-14 00:00:00| 2.1|\n",
      "|  0|2021-09-21 00:00:00| 2.1|\n",
      "|  0|2021-09-28 00:00:00| 2.1|\n",
      "|  0|2021-10-05 00:00:00| 2.1|\n",
      "|  0|2021-10-12 00:00:00| 2.1|\n",
      "|  0|2021-10-19 00:00:00| 2.1|\n",
      "|  0|2021-10-26 00:00:00| 2.1|\n",
      "+---+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT LOC, Date as ds, sum(Sold_bandwidth) as y FROM Sold_bandwidth GROUP BY LOC, ds ORDER BY LOC, ds\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e59b64",
   "metadata": {},
   "source": [
    "Whatever we have done till now in Spak context let us print the plan which is DAG prepared by a spark. So we observe that it is a single RDD function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "516b56dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:16.969723Z",
     "start_time": "2022-04-24T11:36:16.939803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[Date#0,LOC#1L,RING#2L,Sold_bandwidth#3,Utilized_bandwidth#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81862824",
   "metadata": {},
   "source": [
    "# Repartition the Data\n",
    "\n",
    "### Now all the data is in a single partition but now I want it to break into Multiple partitions and for that, we will call the SQL statement on its top and repartition the data based on the LOC column. And we will cache it so that we do not require to fetch data again and again. Machine Learning is an iterative process so we did not want to take data again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "facba63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:17.156225Z",
     "start_time": "2022-04-24T11:36:16.972716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [LOC#1L, ds#97, y#98]\n",
      "   +- InMemoryRelation [LOC#1L, ds#97, y#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Exchange hashpartitioning(LOC#1L, 1), false, [id=#155]\n",
      "            +- *(3) Sort [LOC#1L ASC NULLS FIRST, ds#97 ASC NULLS FIRST], true, 0\n",
      "               +- Exchange rangepartitioning(LOC#1L ASC NULLS FIRST, ds#97 ASC NULLS FIRST, 200), true, [id=#151]\n",
      "                  +- *(2) HashAggregate(keys=[LOC#1L, Date#0], functions=[sum(Sold_bandwidth#3)])\n",
      "                     +- Exchange hashpartitioning(LOC#1L, Date#0, 200), true, [id=#147]\n",
      "                        +- *(1) HashAggregate(keys=[LOC#1L, Date#0], functions=[partial_sum(Sold_bandwidth#3)])\n",
      "                           +- *(1) Project [Date#0, LOC#1L, Sold_bandwidth#3]\n",
      "                              +- *(1) Scan ExistingRDD[Date#0,LOC#1L,RING#2L,Sold_bandwidth#3,Utilized_bandwidth#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOC_part = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['LOC'])).cache()\n",
    "LOC_part.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7063a",
   "metadata": {},
   "source": [
    "### Earlier we saw explain function only give an RDD function but now if we see it has done a lot of hash partitioning. Now we create a UDF function that will have facebook Prophet code. For that, we are importing Pyspark SQL data types to create a schema for our return object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "493b2bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:17.172182Z",
     "start_time": "2022-04-24T11:36:17.159217Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "result_schema = StructType([\n",
    "                  StructField('ds', TimestampType()),\n",
    "                  StructField('LOC', IntegerType()),\n",
    "                  StructField('y', DoubleType()),\n",
    "                  StructField('yhat', DoubleType()),\n",
    "                  StructField('yhat_upper', DoubleType()),\n",
    "                  StructField('yhat_lower', DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87670c",
   "metadata": {},
   "source": [
    "### So we create a result schema which states that the Timestamp is returned schema, Y is a value we are passing, That is the Prophet predicted value, That upper and lower are respective Upper and lower confidence intervals. So when Facebook Prophet predicts the value we can set the confidence Interval. Now from the Pyspark SQL function we will import Pandas UDF and UDF type and define a function that is the same as Pythons typical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a16e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfc295a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:36:17.204096Z",
     "start_time": "2022-04-24T11:36:17.176171Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def forecast_Sold_bandwidth(store_pd):\n",
    "    model = Prophet(interval_width=0.95, seasonality_mode= 'multiplicative', weekly_seasonality=True, yearly_seasonality=True)\n",
    "    model.fit(store_pd)\n",
    "    future_pd = model.make_future_dataframe(periods=5, freq='w')\n",
    "    forecast_pd = model.predict(future_pd)\n",
    "    f_pd = forecast_pd[['ds', 'yhat', 'yhat_upper', 'yhat_lower']].set_index('ds')\n",
    "    st_pd = LOC_pd[['ds', 'LOC', 'y']].set_index('ds')\n",
    "    result_pd = f_pd.join(st_pd, how='left')\n",
    "    result_pd.reset_index(level=0, inplace=True)\n",
    "    result_pd['LOC'] = LOC_pd['LOC'].iloc[0]\n",
    "    return result_pd[['ds', 'LOC', 'y', 'yhat', 'yhat_upper', 'yhat_lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b08701",
   "metadata": {},
   "source": [
    "## # Here I am telling you that it is a grouped Map which means that when we create a Spark UDF then it operates row by row so for each row spark rates are going to execute it but the grouped map allows to be vectorized in a particular set of rows into the pandas UDF. In our case vectorization is based on Store ID so we are going to take a huge chunk of LOC and pass it as a vectorized object and run the Facebook Prophet and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ad3597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-24T11:41:24.568296Z",
     "start_time": "2022-04-24T11:41:24.470557Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\sql\\pandas\\group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 1207, DESKTOP-6O0I7R0, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\t... 37 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3368\\3092987179.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLOC_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforecast_Sold_bandwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark/spark-3.0.3-bin-hadoop2.7/python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 1207, DESKTOP-6O0I7R0, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\t... 37 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "results = (LOC_part.groupby('LOC').apply(forecast_Sold_bandwidth).withColumn('training_date', current_date()))\n",
    "results.cache()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd90a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
